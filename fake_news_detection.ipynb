{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./test_data.csv\")\n",
    "test_data = pd.read_csv(\"./test_data.csv\")\n",
    "\n",
    "# convert `text` column back to list\n",
    "train_data[\"text\"] = train_data[\"text\"].apply(ast.literal_eval)\n",
    "test_data[\"text\"] = test_data[\"text\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Naive Bayes - based only on the content of the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(train_data[\"text\"].apply(lambda words: \" \".join(words)))\n",
    "y = train_data[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_classifier = MultinomialNB()\n",
    "NB_classifier.fit(X, y)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9035385361124576"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = vectorizer.transform(test_data[\"text\"].apply(lambda words: \" \".join(words)))\n",
    "y_pred = NB_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(test_data[\"label\"], y_pred)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91      2037\n",
      "           1       0.96      0.85      0.90      2089\n",
      "\n",
      "    accuracy                           0.90      4126\n",
      "   macro avg       0.91      0.90      0.90      4126\n",
      "weighted avg       0.91      0.90      0.90      4126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_data[\"label\"], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Logistic Regression - based on author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = OneHotEncoder()\n",
    "\n",
    "X = onehot.fit_transform(np.array(train_data[\"author\"]).reshape(-1, 1))\n",
    "y = test_data[\"label\"]\n",
    "\n",
    "LR = LogisticRegression()\n",
    "\n",
    "LR.fit(X, y)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8691226369365003"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = onehot.transform(np.array(test_data[\"author\"]).reshape(-1, 1))\n",
    "y_pred = LR.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(test_data[\"label\"], y_pred)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.87      2037\n",
      "           1       0.86      0.89      0.87      2089\n",
      "\n",
      "    accuracy                           0.87      4126\n",
      "   macro avg       0.87      0.87      0.87      4126\n",
      "weighted avg       0.87      0.87      0.87      4126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_data[\"label\"], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Custom Neural Network - based on both conent and author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140914"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_file = open(\"./data/text_vocab.json\", mode=\"r\")\n",
    "text_vocab = json.load(vocab_file)\n",
    "\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_post(sequence, length):\n",
    "    return sequence + ['<pad>'] * (length - len(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(sequence):\n",
    "    return [text_vocab[word] for word in sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(text_vocab)\n",
    "embedding_dim = 2\n",
    "max_len = max(train_data['text'].str.len())\n",
    "drop_value = 0.2\n",
    "batch_size = 32\n",
    "epoch=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df, batch_size, max_len):\n",
    "    for i in range(0, len(df[\"text\"]), batch_size):\n",
    "        x = [encode_text(padding_post(sequence, max_len)) for sequence in df[\"text\"].iloc[i:i+batch_size]]\n",
    "        y = [label for label in df[\"label\"].iloc[i:i+batch_size]]\n",
    "\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "129/129 - 5s - loss: 0.6142 - accuracy: 0.6842 - 5s/epoch - 38ms/step\n",
      "Epoch 2/10\n",
      "129/129 - 4s - loss: 0.3293 - accuracy: 0.9072 - 4s/epoch - 31ms/step\n",
      "Epoch 3/10\n",
      "129/129 - 4s - loss: 0.1602 - accuracy: 0.9569 - 4s/epoch - 31ms/step\n",
      "Epoch 4/10\n",
      "129/129 - 4s - loss: 0.1008 - accuracy: 0.9716 - 4s/epoch - 31ms/step\n",
      "Epoch 5/10\n",
      "129/129 - 4s - loss: 0.0707 - accuracy: 0.9840 - 4s/epoch - 30ms/step\n",
      "Epoch 6/10\n",
      "129/129 - 4s - loss: 0.0559 - accuracy: 0.9859 - 4s/epoch - 30ms/step\n",
      "Epoch 7/10\n",
      "129/129 - 4s - loss: 0.0419 - accuracy: 0.9908 - 4s/epoch - 30ms/step\n",
      "Epoch 8/10\n",
      "129/129 - 4s - loss: 0.0332 - accuracy: 0.9932 - 4s/epoch - 31ms/step\n",
      "Epoch 9/10\n",
      "129/129 - 4s - loss: 0.0242 - accuracy: 0.9971 - 4s/epoch - 32ms/step\n",
      "Epoch 10/10\n",
      "129/129 - 4s - loss: 0.0189 - accuracy: 0.9981 - 4s/epoch - 32ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "token = Tokenizer(num_words=vocab_size, oov_token='<pad>')\n",
    "token.fit_on_texts(train_data[\"text\"])\n",
    "\n",
    "Trainning_seq = token.texts_to_sequences(train_data[\"text\"])\n",
    "Trainning_pad = pad_sequences(Trainning_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "model=tf.keras.models.Sequential([tf.keras.layers.Embedding(vocab_size, 16, input_length=max_len),\n",
    "                                  tf.keras.layers.GlobalAveragePooling1D(),\n",
    "                                  tf.keras.layers.Dropout(0.3),\n",
    "                                  tf.keras.layers.Dense(32,activation='relu'),\n",
    "                                  tf.keras.layers.Dropout(0.3),\n",
    "                                  tf.keras.layers.Dense(1,activation='sigmoid')])\n",
    "\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),metrics=['accuracy'],optimizer='adam')\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=3)\n",
    "\n",
    "history = model.fit(Trainning_pad, train_data[\"label\"], epochs=epoch, callbacks=[early_stop], verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model compiled\n",
      "start training\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\hurub\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\hurub\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\hurub\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\hurub\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\hurub\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\hurub\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 235, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_14' (type Sequential).\n    \n    Input 0 of layer \"lstm_13\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 2)\n    \n    Call arguments received by layer 'sequential_14' (type Sequential):\n      • inputs=tf.Tensor(shape=(None,), dtype=string)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [83], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mstart training\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     25\u001b[0m     tf\u001b[39m.\u001b[39;49mconvert_to_tensor(test_data[\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m words: \u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(words))\u001b[39m.\u001b[39;49mtolist()),\n\u001b[0;32m     26\u001b[0m     tf\u001b[39m.\u001b[39;49mconvert_to_tensor(train_data[\u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m]),\n\u001b[0;32m     27\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[early_stop]\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmodel trained\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[39m# Evaluate the model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hurub\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file48c1q48g.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\hurub\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\hurub\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\hurub\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\hurub\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\hurub\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\hurub\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 235, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_14' (type Sequential).\n    \n    Input 0 of layer \"lstm_13\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 2)\n    \n    Call arguments received by layer 'sequential_14' (type Sequential):\n      • inputs=tf.Tensor(shape=(None,), dtype=string)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(drop_value))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"model compiled\")\n",
    "\n",
    "# Set up early stopping\n",
    "early_stop = EarlyStopping(patience=3, restore_best_weights=True)\n",
    "\n",
    "print(\"start training\")\n",
    "# Train the model\n",
    "model.fit(\n",
    "    tf.convert_to_tensor(test_data[\"text\"].apply(lambda words: \" \".join(words)).tolist()),\n",
    "    tf.convert_to_tensor(train_data['label']),\n",
    "    epochs=10, batch_size=32, callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "print(\"model trained\")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(test_data['text'], test_data['label'])\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a step-by-step explanation of the model architecture:\n",
    "\n",
    "    tf.keras.layers.Embedding: This layer represents word embeddings. It takes an integer-encoded vocabulary and converts it into dense vector representations. It has vocab_size as the input dimension, 16 as the embedding dimension, and input_length=50 indicates that each input sequence has a length of 50.\n",
    "\n",
    "    tf.keras.layers.GlobalAveragePooling1D: This layer performs average pooling across the sequence dimension. It takes the embedded sequences and calculates the average value for each feature dimension. This reduces the dimensionality of the data.\n",
    "\n",
    "    tf.keras.layers.Dense: This layer is a fully connected (dense) layer with 32 units/neurons. It applies the ReLU activation function, which introduces non-linearity to the model.\n",
    "\n",
    "    tf.keras.layers.Dropout: This layer applies dropout regularization to prevent overfitting. It randomly sets a fraction (0.3) of input units to 0 at each update during training time.\n",
    "\n",
    "    tf.keras.layers.Dense: This is the final output layer with a single neuron. It uses the sigmoid activation function to produce a binary classification output (0 or 1), indicating the likelihood of a sample belonging to a specific class (e.g., fake news detection).\n",
    "\n",
    "Overall, this model architecture consists of an embedding layer, a pooling layer, two dense layers with activation functions, and a dropout layer. It is suitable for tasks like sentiment analysis or binary classification, where the input consists of text sequences of fixed length."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f8c03f552119a08b050861f96dc0d6633087e20f29f534aef895879f6be696f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
